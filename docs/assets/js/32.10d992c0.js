(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{66:function(t,a,e){"use strict";e.r(a);var s=e(1),_=function(t){t.options.__data__block__={po:"popopo"}},n=Object(s.a)({},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"기계가-통계적-학습을-통해-판단을-할-수-있지-않을까"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#기계가-통계적-학습을-통해-판단을-할-수-있지-않을까","aria-hidden":"true"}},[t._v("#")]),t._v(" 기계가 통계적 학습을 통해 판단을 할 수 있지 않을까")]),t._v(" "),e("article-info"),e("p",[t._v("우리는 확률과 결정 이론을 통해 불확실성을 다룰 수 있음을 보았다. 그런데 그렇게 하기 위해서는, 우선 세계의 사건을 관찰하고 그에 대해 통계적 이론을 세울 수 있어야 한다. 우리는 특히 베이지안 네트워크에서의 두 종류 통계적 이론을 배울 것이다. 각각의 노드에서 확률 그 자체와, 조건적 독립(conditional independence)에 따른 인과적 구조를.")]),t._v(" "),e("h2",{attrs:{id:"각-node에서의-통계적-학습"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#각-node에서의-통계적-학습","aria-hidden":"true"}},[t._v("#")]),t._v(" 각 node에서의 통계적 학습")]),t._v(" "),e("ol",[e("li",[t._v("자연 과학에서의 경우")])]),t._v(" "),e("p",[t._v("다음과 같은 확률로, 각 봉투에 사탕의 비율이 다르게 담겨있다고 하자.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("h1: 100% 체리\nh2: 75% 체리 + 25% 라임\nh3: 50% 체리 + 50% 라임\nh4: 25% 체리 + 75% 라임\nh5: 100% 라임\n")])])]),e("p",[t._v("실제 우리가 선택한 봉투들은, D1, D2, D3, ... 와 같이랜덤한 개별 데이터로 나타나고, 가설적 봉투 타입 H는 실제로나타나지 않는다. 말하자면, 우리가 어떤 봉투를 뽑을 확률을지정해도, 이것이 직접 관찰될 수는 없고, 관찰한 데이터에대해 사후적인 분석을 통해 나타난다. 행위자(agent)에게주어진 임무는, 다음 사탕 봉투의 맛을 예측하는 것이라고해보자.")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("베이지안 학습"),e("br"),t._v("\n단순히 주어진 데이터를 기본으로 각 가설의 확률을 계산하고, 그에 기반하여 다음 데이터를 예측한다. 즉, 한 가지 최고의 가설을 채택하는 것이 아니라, 모든 가설들이 각각의 확률만큼 가중되어 예측에 쓰인다.")]),t._v(" "),e("p",[t._v("D가 모든 데이터를 나타나고, 관찰된 값이 d라 하자. 우리가 모르는 값 X에 대해 예측을 한다고 하자. 각각의 가설이 X에 대한 통계적 분포를 결정한다고 가정한다면,")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P( X|d )\n= Sigma_i ( P( X|d,hi )P( hi|d ) )\n= Sigma_i ( P( X|hi )P( hi|d ) )\n")])])]),e("p",[t._v("가 성립한다. 각각의 데이터의 경우 hi가 나타날 확률에 hi에서 X가 나타날 확률을 곱한 것들의 합.")]),t._v(" "),e("p",[t._v('이 등식은, 각각의 가설이 가중된 평균으로부터 예측이 세워짐을 보여준다. 우리는 미래의 데이터(future data)에 대해 최고의(best) 예측을 하고 싶은 것인데, 그럼 먼저 "future data"가 무엇이고 "best"가 무엇을 뜻하는지를 정의해야 한다. 고로 우리는 stationary assumption을 일단 하자.')]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("stationary assumption\n사례들에 대해 시간과 별개로 stationary한 확률 분포가 있다고 가정하자. 각각의 사례 데이터(example data point)는 어떠한 랜덤 변수 Ej이고, 관찰된 값 ej는 그 분포로부터 추출된 것이며, 그전 예시와 독립적이다.\n")])])]),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P ( Ej | Ej-1, Ej-2, Ej-3, ...) = P ( Ej )\n")])])]),e("p",[t._v("즉, 그전 어떤 분포가 나타났는지와 상관없이 다음 분포는 독립적이다.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P( Ej ) = P( Ej-1 ) = P( Ej-2 ) = ...\n")])])]),e("p",[t._v("즉, 각각의 사건들은 독립적일뿐 아니라 항상 동일하게(identical)하게 나타난다. 정리하면, Independent하며 Identically Distributed된다. (i.i.d. 가정) 이에 의해, 과거 사건과 미래의 사건이 연결된다.")]),t._v(" "),e("p",[t._v("베이지안 학습의 특징은, 제대로 된 가설(true hypothesis)에 기초한 고정된 사전적 사건들에 따라, 어떠한 정상적 조건 아래에서는 어떠한 잘못된 가설의 사후적 가능성이 점차 사라진다는 것이다. (초기 사건들을 보고 후기 사건들을 점차 잘 예측)")]),t._v(" "),e("p",[t._v("데이터를 가지고 베이지안 학습을 한다. 그런데 만약 데이터의 숫자가 무한히 많아지면, "),e("code",[t._v("P(X|d)")]),t._v(", 즉 다음에 어떠한 사건 X가 무수히 많은 사건에 대해 발생할 확률이 얼마라고 구한다면, 그렇다면 "),e("code",[t._v("P(X | hi)")]),t._v(", 다음 사건에서 같은 사건이 일어날 확률은, 우리가 이미 알게 된 확률과 동일하다. "),e("code",[t._v("P( P(X|d) -> P(X|hi) ) = 1")]),t._v(".")])]),t._v(" "),e("li",[e("p",[t._v("최대가능성 학습 (Maximum-likelihood learning)")]),t._v(" "),e("p",[t._v("최대가능성 학습은 베이지안 학습으로부터 가설 공간(the psace of hypotheses)에 대해 동일한 이전 사건(a uniform prior)을 가정하여 얻어진다.")]),t._v(" "),e("p",[t._v("예를 들어 완전히 알려지지 않은 비율의, 새로운 사탕 봉투를 산다고 하자. 그럼 비율은 0~1의 어떠한 정도도 될 수 있다. 그러면 우리는 체리 사탕의 비율을 말하는 가설의 연속체 h_theta를 얻게 된다. 우리가 N개의 사탕을 개봉한다고 하자. c가 체리의 개수이고, l=N-c가 라임의 개수인 경우")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P ( d|h_theta )\n= Product_from_j_to_N( P( dj|h_theta ) )\n= theta^c * ( l-theta)^l\n")])])]),e("p",[t._v("의 식이 특정한 데이터 집합의 확률로 나타난다. 최대가능성 가설은, 이 수식을 최대화하는 theta의 값에 의해 주어진다. 고로 우리는 "),e("code",[t._v("theta = c/N")]),t._v("를 얻게 된다. 최대가능성 가설 hML은 지금까지 까본 사탕에서 관찰된 비율과 동일하게 실제 체리의 비율을 추론한다.")])]),t._v(" "),e("li",[e("p",[t._v("기계는 각 노드에서 확률을 학습할 수 있는가")])])]),t._v(" "),e("h3",{attrs:{id:"strong-law-of-large-number"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#strong-law-of-large-number","aria-hidden":"true"}},[t._v("#")]),t._v(" Strong Law of Large Number")]),t._v(" "),e("p",[t._v("X1, X2, ...가 i.i.d.한 무한한 수열이며, E(X) = mu로 같다고 하자. 그러면 무한히 많은 추출을 하면 결국 참된 평균으로 수렴하게 된다.")]),t._v(" "),e("p",[t._v("그러나 지진을 고려해보자. At가 LA에서 t의 시점에 지진이 발생하는 사건이며, Xt는 At가 발생하는 경우 1이고 아닌 경우 0이라 하자. 그럼 X1, X2, ...는 iid 분포가 아니라고 가정해도 된다.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("Borel-Cantelli lemma\nif Sigma_(t=1 to infinity) P(At) < infinity\nthen P ( At i.o. ) = 0\n")])])]),e("p",[t._v("즉, 모든 At의 합이 무한이 아니려면, 마지막에는 확률이 0에 가까워야 한다. 그러므로, 완전히 지진이 멈추기 전까지는, At의 장기적인 합은 무한이 될 것이다.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("if A1, A2, ...가 짝마다 독립적이고\n모든 P(At)의 합이 무한이라면,\nprobability one의 경우 1/n(모든 Xt의 합은)\n1/n( P(At)의 합이 될 것이다. )\n")])])]),e("p",[t._v("즉, 실제 데이터의 상대적인 빈도수는 변화하는 확률의 평균에 수렴하지, 실제 확률로 수렴하지 않는다. 예를 들어, 실제 지진의 확률이 1/2와 1/3 사이를 왔다갔다 하고 있다고 한다. 그럼 홀수의 경우 1/2이고, 다음 경우 실제로는 1/3이 될 것이다. 그럼 장기적으로는 5/12가 될 것이다. 그러나 사실 1/2나 1/3이어야 한다.")]),t._v(" "),e("p",[t._v("물론, 실제 확률의 평균에는 한도가 존재한다. 장기적인 경험적 빈도수는 이 limit에 수렴하지만, 실제 참인 가능성에 수렴하지 않는다.")]),t._v(" "),e("p",[t._v("데이터로부터 실제 확률을 학습하기 위해서는, 우리;는 사건들을 1/2인 경우와 1/3인 경우를 나눠야 한다. 그러나 이것은 실제 확률이 어떤지를 알아야만 가능하다. 고로 순환적이다.")]),t._v(" "),e("p",[t._v("여러 분포들이 섞여있는 경우, 예를 들어 사탕의 비율에 대해 다음과 같은 두 가지 경우를 고려해보자.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("h0 : 3/4\nh1 : 1/4\n")])])]),e("p",[t._v("예를 들어 내가 odd1 시점에 3/4를 뽑고, even1시점에 1/4를 뽑았다고 하자. 이런 식으로 계속 odd와 even 시점에 각각의 경우를 뽑는다면, 확률은 결국 1/2( 3/4 + 1/4 )로 수렴할 것이다.")]),t._v(" "),e("p",[t._v("그런데, 참인 분포를 따르는 데이터를 모아야 학습을 할 수 있다 하면, 참인 분포를 어떻게 알 것인가? 그렇다면 우리가 학습을 해야 참인 분포를 판단할 수 있을 것이다. 그럼 결국 순환적으로 둘이 서로 의존하기 때문에 데이터를 구할 수 없게 된다.")]),t._v(" "),e("ol",[e("li",[t._v("사회과학에서의 경우")])]),t._v(" "),e("p",[t._v("경제학에서의 경우 행위자의 주관적 믿음이 중요한 역할을 한다.")]),t._v(" "),e("ol",[e("li",[t._v("예시")])]),t._v(" "),e("p",[t._v("고립된 시장(isolated market)에서의 저장 불가능한 재화의 고정된 생산 지체(fixed production lag)을 가정하고, 단기 가격 변화를 고려해보자.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("Ct = c - Beta * p_t (수요)\nPt = s + Alpha * p_te + u_t (공급)\nPt = Ct (균형점)\n")])])]),e("p",[t._v("Pt는 생산 기간 동안에서의 생산된 수,"),e("br"),t._v("\nCt는 시점 t에 소비된 수량"),e("br"),t._v("\np_t는 시점 t에 시장가격"),e("br"),t._v("\np_te는 시점 t에 기대되는 시장가격 ( t-1까지의 정보에 기초한 추측 )"),e("br"),t._v("\nu_t는 오류항(error term)")]),t._v(" "),e("p",[t._v("그럼 균형 가격은 주관적 믿음의 함수가 될 것이다. 즉, 자연과학과 달리 사회과학에서는 주관적 믿음이 역할을 한다. (예를 들어 내일 비가 올 확률 vs 내년 경제 성장률)")]),t._v(" "),e("p",[t._v("고로 사회과학 모델의 가능성을 학습하기 위해서는 중요한 것은 주관적 가능성이다. 그러나, 각각의 주관적 가능성을 얻어내기 위해서는 AI는 가설적 요소들을 포함하는, 각 개인의 선호 체계를 배워야 한다. 모델의 대상이 거시경제적 수준으로 확대되면, 학습을 위해 필요한 정의역의 크기는 거의 폭발적으로 증가하게 된다. 그래서 AI가 주관적 확률을 학습하는 것은 거의 불가능하다.")]),t._v(" "),e("ol",{attrs:{start:"3"}},[e("li",[t._v("인과적 구조의 확률적 학습")])]),t._v(" "),e("p",[t._v("확률적 인관의 기본적 아이디어는, 원인들이 그들의 결과에 대해 확률을 유발한다는 것이다. 흡연이 일부의 경우 폐암을 유발하지 않지만, 흡연은 폐암의 확률을 높인다.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("(PR) C는 P( E|C ) > P( E|~C )인 경우 E의 원인이다\n")])])]),e("p",[t._v("즉, C가 발생할 경우 E의 발생 확률이 높아진다면 인과라고 할 수 있다. 그런데, C와 E가 둘 다 다른 제3의 원인에 의해 유발된다면, 위의 정의를 만족함에도 그럼 상관관계와 인과관계가 구분되지 않는다.")]),t._v(" "),e("p",[t._v("예를 들어 기압계와 폭풍은 상관 관계를 가지지만, 인과 관계를 가지는 것은 아니고, 기압이라는 공통 원인에 의해 발생하는 것이다. 고로 공통원인이 무엇인지 포착하기 위해 screening off가 필요하다.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P( E|A&C ) = P( E|C ) -> C is said to screen A off from E\n")])])]),e("p",[t._v("A와 C가 동시에 발생했을 때 E가 발생할 확률과 C가 발생했을 때 E가 발생할 확률이 같다면, 공통 원인인 것으로 하자.")]),t._v(" "),e("p",[t._v("P( A&C ) > 0이라면,")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P( A&E | C ) = P(A | C) * P( E|C )\n")])])]),e("p",[t._v("즉 A와 E는 C에 대해 확률적으로 독립적인 조건절이다. 이는 그 자식들을 다른 모든 변수들로부터 screen off하는 부모로 베이지안 네트워크에 표현된다.")]),t._v(" "),e("p",[t._v("Ct는 Et'의 원인이다. iff")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("i) t < t'\nii) P( Et'|Ct ) > P( Et'|~Ct )\niii) t와 동시에, 혹은 그 이전에 발생하며, Et'를 Ct로부터 screen off하는 Dt''와 같은 사건은 없다.\n")])])]),e("p",[t._v("즉, t는 t'에 대해 선행하며, Ct가 발생할 경우 ~Ct인 경우보다 Et'가 발생할 확률이 더 높아진다. 베이지안 네트워크는 어떤 유한한 n에 대해, probabilistic domain의 크기를 O(2^n)에서 O(n)으로 줄인다. 그러나 이제 이 공간은 거의 무한으로 확장된다.")]),t._v(" "),e("h3",{attrs:{id:"심슨의-역설"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#심슨의-역설","aria-hidden":"true"}},[t._v("#")]),t._v(" 심슨의 역설")]),t._v(" "),e("p",[t._v("흡연이 시골에 사는 것과 높은 상관 관계를 가진다고 하자. 그런데 도시의 오염이 더 강한 폐암의 발병 요인이라고 하자. 그럼 흡연자들이 폐암에 걸릴 확률이 더 적게 나타날 수 있다. C가 흡연, B가 시골에 사는 것, E가 폐암이라 하면")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P( E|C ) < P( E|~C )\n")])])]),e("p",[t._v("가 된다. 흡연이 폐암의 원인임에도. 그래서 E를 B에 따라 조건화한다면 반대가 된다.")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("P( E|C&B ) > P( E|~C&B )\n")])])]),e("p",[t._v("고로 우리는 C가 E의 원인이라 말하기 위해서는")]),t._v(" "),e("div",{staticClass:"language-text extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("모든 배경 맥락 B에 대해 P( E|C&B ) > P( E|~C&B )\n")])])]),e("p",[t._v("가 성립해야 한다고 말할 수 있다. 그런데 어떤 것이 배경 맥락인지 어떻게 알고, 이 모든 것들을 어떻게 수집할 수 있는가? 고로 아마도 AI가 베이지안 네트워크를 통해 인과를 익히기란 쉽지 않을 것이다.")])],1)},[],!1,null,null,null);"function"==typeof _&&_(n),n.options.__file="12-statistical-learning.md";a.default=n.exports}}]);